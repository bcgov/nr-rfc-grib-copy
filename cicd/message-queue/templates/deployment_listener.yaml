kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
    app.openshift.io/vcs-uri: 'https://github.com/bcgov/nr-rfc-grib-copy'
  name:  {{ .Values.app.name}}-{{ .Values.app.zone }}-{{ .Values.app.component }}
  labels:
    app: {{ .Values.app.name }}-{{ .Values.app.zone }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: {{ .Values.app.name }}-{{ .Values.app.zone }}
      #env: {{ .Values.env }}
      #deployment: {{ .Values.app_name }}-deploy
  template:
    metadata:
      labels:
        app: {{ .Values.app.name }}-{{ .Values.app.zone }}
        deployment: {{ .Values.app.name}}-{{ .Values.app.zone }}-{{ .Values.app.component }}
    spec:
      # imagePullSecrets:
      #   - name: deployer-token-klpcz
      volumes:
        - name: {{ .Values.app.name }}-{{ .Values.app.name }}-{{ .Values.app.component }}
          persistentVolumeClaim:
            claimName: {{ .Values.app.name }}-{{ .Values.app.name }}-{{ .Values.app.component }}
      containers:
        - name: {{ .Values.app.name }}
          image: >-
            {{ .Values.image.image_registry }}/{{ .Release.Namespace }}/{{ .Values.app.name}}-{{ .Values.app.zone }}-{{ .Values.app.component }}:{{ .Values.image.image_tag }}
          #{{ .Values.app.name}}-{{ .Values.app.zone }}-{{ .Values.app.component }}:{{ .Values.image.image_tag }}
          livenessProbe:
            httpGet:
              path: /healthz
              port: {{ .Values.app.api_port }}
            initialDelaySeconds: 60
            periodSeconds: 15
            failureThreshold: 10

          readinessProbe:
            httpGet:
              path: /healthz
              port: {{ .Values.app.api_port }}
            initialDelaySeconds: 30
            periodSeconds: 15
            failureThreshold: 10

          ports:
            - containerPort: {{ .Values.app.api_port }}
              protocol: TCP
          imagePullPolicy: Always
          env:
            - name: DB_FILE_PATH
              value: "sqlite:////data/event_database.db"
            - name: AMPQ_USERNAME
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.app.name }}-{{ .Values.app.name }}-{{ .Values.app.component }}
                  key: ampq_username
            - name: AMPQ_PASSWORD
              valueFrom:
                secretKeyRef:
                  name:  {{ .Values.app.name }}-{{ .Values.app.name }}-{{ .Values.app.component }}
                  key: ampq_password
            - name: AMPQ_DOMAIN
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.app.name }}-{{ .Values.app.name }}-{{ .Values.app.component }}
                  key: ampq_server
            - name: GH_ORG
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.app.name }}-{{ .Values.app.name }}-{{ .Values.app.component }}-{{ .Values.github_secrets.name }}
                  key: gh-org
            - name: GH_REPO
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.app.name }}-{{ .Values.app.name }}-{{ .Values.app.component }}-{{ .Values.github_secrets.name }}
                  key: gh-repo
            - name: GH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.app.name }}-{{ .Values.app.name }}-{{ .Values.app.component }}-{{ .Values.github_secrets.name }}
                  key: gh-access-token
            - name: ZONE
              value: {{ .Values.app.zone }}

          volumeMounts:
            - name: {{ .Values.app.name }}-{{ .Values.app.name }}-{{ .Values.app.component }}
              mountPath: {{ .Values.app.pvc_mount_point }}
          resources:
            limits:
              cpu: 200m
              memory: 1100Mi
            requests:
              cpu: 50m
              memory: 125Mi

      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirst
      securityContext: {}
      schedulerName: default-scheduler
      imagePullSecrets: []
  strategy:
    # currently caching the events in a sqlite database, so we need to recreate
    # the pod otherwise it will raise and error when two pods are trying to
    # access the same volume eventually can replace with a proper database like
    # postgres
    type: Recreate

    # type: RollingUpdate
    # rollingUpdate:
    #   maxSurge: 25%
    #   maxUnavailable: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
  paused: false
